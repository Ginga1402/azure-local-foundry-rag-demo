{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfcc416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports for loading PDFs, splitting text, creating embeddings, and storing vectors\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d96654ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports for environment variables, LLM, prompts, and local model manager\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from foundry_local import FoundryLocalManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3184887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up embedding model name and configuration\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "# Optional model settings (e.g., GPU)\n",
    "# model_kwargs = {'device': 'cuda'}\n",
    "\n",
    "# Encoding options for the embeddings\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# Create the HuggingFace embeddings object\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    # model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70c497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF documents using the loaders list\n",
    "loaders = [\n",
    "\n",
    "    PyPDFLoader(r\"Visa The rise of Agentic Commerce.pdf\")\n",
    "\n",
    "]\n",
    "documents = []\n",
    "for loader in loaders:\n",
    "    documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c2ea4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Chroma Vector Store Created: <langchain_chroma.vectorstores.Chroma object at 0x00000119398FB8E0>\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Split documents into smaller text chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(documents=documents)\n",
    "\n",
    "# Create and store embeddings in Chroma\n",
    "vector_store = Chroma.from_documents(\n",
    "    texts,\n",
    "    embeddings,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "    persist_directory=\"stores/data_cosine\"\n",
    ")\n",
    "\n",
    "print(\"*\" * 100)\n",
    "print(\"Chroma Vector Store Created:\", vector_store)\n",
    "print(\"*\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95dac4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever to fetch top 2 similar chunks\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e622cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model alias and create the local model manager\n",
    "alias = \"qwen2.5-0.5b\"\n",
    "manager = FoundryLocalManager(alias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a02c8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat model using the local model manager settings\n",
    "llm = ChatOpenAI(\n",
    "    model=manager.get_model_info(alias).id,\n",
    "    base_url=manager.endpoint,\n",
    "    api_key=manager.api_key,\n",
    "    temperature=0.3,\n",
    "    streaming=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b16bff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile={} client=<openai.resources.chat.completions.completions.Completions object at 0x00000119398E5E70> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000119398E5FF0> root_client=<openai.OpenAI object at 0x00000119398E5FC0> root_async_client=<openai.AsyncOpenAI object at 0x000001193A35BAF0> model_name='qwen2.5-0.5b-instruct-cuda-gpu:4' temperature=0.3 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='http://127.0.0.1:61031/v1'\n"
     ]
    }
   ],
   "source": [
    "# Print the LLM configuration\n",
    "print(llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2336ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create the RAG function\n",
    "def ask_question(question: str, retriever, llm):\n",
    "    # Prompt template\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Use the following pieces of document to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Only return the helpful answer below and nothing else.\"\"\"\n",
    "        ),\n",
    "        (\"human\", \"Document: {document}\\nQuestion: {question}\\n\\nHelpful answer:\")\n",
    "    ])\n",
    "\n",
    "    # Combine prompt with LLM\n",
    "    rag_chain = prompt | llm\n",
    "\n",
    "    # Retrieve documents\n",
    "    docs = retriever.invoke(question)\n",
    "\n",
    "    # Run chain\n",
    "    response = rag_chain.invoke({\"document\": docs, \"question\": question})\n",
    "\n",
    "    # Return only content\n",
    "    return response.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aeea80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An AI Agent is defined by its ability to perform tasks autonomously or semi-autonomously using artificial intelligence models to make decisions following reasoning frameworks and leveraging tools to go beyond their training data.\n"
     ]
    }
   ],
   "source": [
    "answer = ask_question(\"What defines an AI Agent?\", retriever, llm)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cbe8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
